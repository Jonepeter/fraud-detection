{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering for Fraud Detection\n",
    "\n",
    "This notebook performs data preprocessing and feature engineering on the fraud detection datasets:\n",
    "1. Fraud_Data.csv - E-commerce transaction data\n",
    "2. IpAddress_to_Country.csv - IP to country mapping\n",
    "3. creditcard.csv - Bank transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Import custom modules\n",
    "from preprocessing import (\n",
    "    handle_missing_values, clean_data, encode_categorical_features,\n",
    "    scale_features, handle_class_imbalance, convert_ip_to_int\n",
    ")\n",
    "from feature_engineering import (\n",
    "    add_time_features, add_transaction_features,\n",
    "    merge_ip_country_data, add_amount_features\n",
    ")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the datasets\n",
    "fraud_data = pd.read_csv('../data/Fraud_Data.csv')\n",
    "ip_country = pd.read_csv('../data/IpAddress_to_Country.csv')\n",
    "creditcard = pd.read_csv('../data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Fraud_Data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clean data\n",
    "fraud_data = clean_data(fraud_data)\n",
    "\n",
    "# Handle missing values\n",
    "fraud_data = handle_missing_values(fraud_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Merge with IP-to-country data\n",
    "fraud_data = merge_ip_country_data(fraud_data, ip_country)\n",
    "\n",
    "# Check the merged data\n",
    "fraud_data[['ip_address', 'country']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Add time-based features\n",
    "fraud_data = add_time_features(fraud_data)\n",
    "\n",
    "# Add transaction features\n",
    "fraud_data = add_transaction_features(fraud_data)\n",
    "\n",
    "# Check the new features\n",
    "fraud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Encode categorical features\n",
    "categorical_cols = ['source', 'browser', 'sex', 'country']\n",
    "fraud_data = encode_categorical_features(fraud_data, categorical_cols)\n",
    "\n",
    "# Check the encoded features\n",
    "fraud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Drop unnecessary columns\n",
    "cols_to_drop = ['user_id', 'device_id', 'ip_address', 'signup_time', 'purchase_time']\n",
    "fraud_data = fraud_data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Check the final dataset\n",
    "fraud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scale numerical features\n",
    "fraud_data, scaler_fraud = scale_features(fraud_data)\n",
    "\n",
    "# Check the scaled features\n",
    "fraud_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the preprocessed data\n",
    "fraud_data.to_csv('../data/fraud_data_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess creditcard.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clean data\n",
    "creditcard = clean_data(creditcard)\n",
    "\n",
    "# Handle missing values\n",
    "creditcard = handle_missing_values(creditcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Add time-based features\n",
    "creditcard = add_time_features(creditcard)\n",
    "\n",
    "# Add amount-based features\n",
    "creditcard = add_amount_features(creditcard)\n",
    "\n",
    "# Check the new features\n",
    "creditcard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scale numerical features\n",
    "creditcard, scaler_cc = scale_features(creditcard)\n",
    "\n",
    "# Check the scaled features\n",
    "creditcard.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the preprocessed data\n",
    "creditcard.to_csv('../data/creditcard_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare Fraud_Data for modeling\n",
    "X_fraud = fraud_data.drop('class', axis=1)\n",
    "y_fraud = fraud_data['class']\n",
    "\n",
    "# Prepare creditcard for modeling\n",
    "X_cc = creditcard.drop('Class', axis=1)\n",
    "y_cc = creditcard['Class']\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Fraud_Data: X shape = {X_fraud.shape}, y shape = {y_fraud.shape}\")\n",
    "print(f\"Creditcard: X shape = {X_cc.shape}, y shape = {y_cc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fraud_Data\n",
    "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.2, random_state=42, stratify=y_fraud\n",
    ")\n",
    "\n",
    "# Creditcard\n",
    "X_train_cc, X_test_cc, y_train_cc, y_test_cc = train_test_split(\n",
    "    X_cc, y_cc, test_size=0.2, random_state=42, stratify=y_cc\n",
    ")\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Fraud_Data: X_train shape = {X_train_fraud.shape}, X_test shape = {X_test_fraud.shape}\")\n",
    "print(f\"Creditcard: X_train shape = {X_train_cc.shape}, X_test shape = {X_test_cc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle class imbalance for Fraud_Data\n",
    "X_train_fraud_resampled, y_train_fraud_resampled = handle_class_imbalance(\n",
    "    X_train_fraud, y_train_fraud, method='smote', sampling_strategy=0.1\n",
    ")\n",
    "\n",
    "# Handle class imbalance for Creditcard\n",
    "X_train_cc_resampled, y_train_cc_resampled = handle_class_imbalance(\n",
    "    X_train_cc, y_train_cc, method='smote', sampling_strategy=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the train-test split data\n",
    "import joblib\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save Fraud_Data splits\n",
    "joblib.dump(X_train_fraud, '../data/processed/X_train_fraud.pkl')\n",
    "joblib.dump(X_test_fraud, '../data/processed/X_test_fraud.pkl')\n",
    "joblib.dump(y_train_fraud, '../data/processed/y_train_fraud.pkl')\n",
    "joblib.dump(y_test_fraud, '../data/processed/y_test_fraud.pkl')\n",
    "joblib.dump(X_train_fraud_resampled, '../data/processed/X_train_fraud_resampled.pkl')\n",
    "joblib.dump(y_train_fraud_resampled, '../data/processed/y_train_fraud_resampled.pkl')\n",
    "\n",
    "# Save Creditcard splits\n",
    "joblib.dump(X_train_cc, '../data/processed/X_train_cc.pkl')\n",
    "joblib.dump(X_test_cc, '../data/processed/X_test_cc.pkl')\n",
    "joblib.dump(y_train_cc, '../data/processed/y_train_cc.pkl')\n",
    "joblib.dump(y_test_cc, '../data/processed/y_test_cc.pkl')\n",
    "joblib.dump(X_train_cc_resampled, '../data/processed/X_train_cc_resampled.pkl')\n",
    "joblib.dump(y_train_cc_resampled, '../data/processed/y_train_cc_resampled.pkl')\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(scaler_fraud, '../data/processed/scaler_fraud.pkl')\n",
    "joblib.dump(scaler_cc, '../data/processed/scaler_cc.pkl')\n",
    "\n",
    "print(\"Data splits saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary of Preprocessing Steps\n",
    "\n",
    "### Fraud_Data.csv\n",
    "1. Cleaned data by removing duplicates and correcting data types\n",
    "2. Handled missing values using imputation\n",
    "3. Merged with IP-to-country data for geolocation analysis\n",
    "4. Added time-based features (hour_of_day, day_of_week, time_since_signup)\n",
    "5. Added transaction features (user_transaction_count, time_since_last_transaction, etc.)\n",
    "6. Encoded categorical features (source, browser, sex, country)\n",
    "7. Dropped unnecessary columns (user_id, device_id, ip_address, signup_time, purchase_time)\n",
    "8. Scaled numerical features\n",
    "9. Split data into training and testing sets\n",
    "10. Handled class imbalance using SMOTE\n",
    "\n",
    "### creditcard.csv\n",
    "1. Cleaned data by removing duplicates and correcting data types\n",
    "2. Handled missing values using imputation\n",
    "3. Added time-based features\n",
    "4. Added amount-based features\n",
    "5. Scaled numerical features\n",
    "6. Split data into training and testing sets\n",
    "7. Handled class imbalance using SMOTE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}